{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9f095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2ac177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "x = cp.arange(6).reshape(2, 3)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a9f416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum(axis=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25171220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "n = np.array([1, 2, 3])\n",
    "c = cp.asarray(n)\n",
    "\n",
    "assert type(c) == cp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "545d40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cp.array([1, 2, 3])\n",
    "n = cp.asnumpy(c)\n",
    "assert type(n) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba402cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "xp = cp.get_array_module(x)\n",
    "assert xp == np\n",
    "\n",
    "x = cp.array([1, 2, 3])\n",
    "xp = cp.get_array_module(x)\n",
    "assert xp == cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051e41be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not using GPU\n",
      "[1/5] train_loss: 2.2670746636390686, accuracy: 0.1744\n",
      "test_loss: 2.2247932720184327, accuracy: 0.1748\n",
      "elapse times : 7.385754823684692\n",
      "[2/5] train_loss: 2.1723104294141136, accuracy: 0.36628333333333335\n",
      "test_loss: 2.104503791332245, accuracy: 0.517\n",
      "elapse times : 6.0160486698150635\n",
      "[3/5] train_loss: 2.0241631187995277, accuracy: 0.5096\n",
      "test_loss: 1.9135946869850158, accuracy: 0.4951\n",
      "elapse times : 6.045778512954712\n",
      "[4/5] train_loss: 1.78109676361084, accuracy: 0.6079166666666667\n",
      "test_loss: 1.6147808802127839, accuracy: 0.6482\n",
      "elapse times : 6.117432355880737\n",
      "[5/5] train_loss: 1.4714031920830408, accuracy: 0.6770833333333334\n",
      "test_loss: 1.3059992212057114, accuracy: 0.7352\n",
      "elapse times : 6.076480150222778\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import mytorch\n",
    "import mytorch.functions as F\n",
    "from mytorch import optimizers, DataLoader\n",
    "from mytorch.models import MLP\n",
    "\n",
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "\n",
    "hidden_size = 1000\n",
    "train_set = mytorch.datasets.MNIST(train=True)\n",
    "test_set = mytorch.datasets.MNIST(train=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, hidden_size, 10))\n",
    "optim = optimizers.SGD(model.parameters)\n",
    "\n",
    "print(\"not using GPU\")\n",
    "    \n",
    "train_loss_saver = []\n",
    "test_loss_saver = []\n",
    "train_acc_saver = []\n",
    "test_acc_saver = []\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    start = time.time()\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    for x, t in train_loader:\n",
    "        pred = model(x)\n",
    "        loss = F.softmax_cross_entropy(pred, t)\n",
    "        acc = F.accuracy(pred, t)\n",
    "        \n",
    "        model.zerograd()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "    train_loss_saver.append((epoch, sum_loss / len(train_set)))\n",
    "    train_acc_saver.append((epoch, sum_acc / len(train_set)))\n",
    "    print(f\"[{epoch+1}/{max_epoch}] train_loss: {sum_loss / len(train_set)}, \"\n",
    "          f\"accuracy: {sum_acc / len(train_set)}\")\n",
    "    \n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with mytorch.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            pred = model(x)\n",
    "            loss = mytorch.functions.softmax_cross_entropy(pred, t)\n",
    "            acc = mytorch.functions.accuracy(pred, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "        test_loss_saver.append((epoch, sum_loss / len(test_set)))\n",
    "        test_acc_saver.append((epoch, sum_acc / len(test_set)))\n",
    "        print(f\"test_loss: {sum_loss / len(test_set)}, \"\n",
    "              f\"accuracy: {sum_acc / len(test_set)}\")\n",
    "    \n",
    "    print(f\"elapse times : {time.time() - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf09dfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n",
      "[1/5] train_loss: 2.2655182472719306, accuracy: 0.17836666666666667\n",
      "test_loss: 2.224961286230049, accuracy: 0.1358\n",
      "elapse times : 5.637044906616211\n",
      "[2/5] train_loss: 2.1678672456258536, accuracy: 0.36741666666666667\n",
      "test_loss: 2.097939332704492, accuracy: 0.4365\n",
      "elapse times : 5.357033014297485\n",
      "[3/5] train_loss: 2.013332341243734, accuracy: 0.5107833333333334\n",
      "test_loss: 1.8958331486127955, accuracy: 0.643\n",
      "elapse times : 5.3505449295043945\n",
      "[4/5] train_loss: 1.7633395323662284, accuracy: 0.6083333333333333\n",
      "test_loss: 1.5931521402023174, accuracy: 0.6747\n",
      "elapse times : 5.3990960121154785\n",
      "[5/5] train_loss: 1.4502346608947838, accuracy: 0.6776833333333333\n",
      "test_loss: 1.2931785370676505, accuracy: 0.6486\n",
      "elapse times : 5.393986940383911\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import mytorch\n",
    "import mytorch.functions as F\n",
    "from mytorch import optimizers, DataLoader\n",
    "from mytorch.models import MLP\n",
    "\n",
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "\n",
    "hidden_size = 1000\n",
    "train_set = mytorch.datasets.MNIST(train=True)\n",
    "test_set = mytorch.datasets.MNIST(train=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, hidden_size, 10))\n",
    "optim = optimizers.SGD(model.parameters)\n",
    "\n",
    "if mytorch.cuda.gpu_enable:\n",
    "    print(\"using GPU\")\n",
    "    train_loader.to_gpu()\n",
    "    test_loader.to_gpu()\n",
    "    model.to_gpu()\n",
    "\n",
    "    \n",
    "train_loss_saver = []\n",
    "test_loss_saver = []\n",
    "train_acc_saver = []\n",
    "test_acc_saver = []\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    start = time.time()\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    for x, t in train_loader:\n",
    "        pred = model(x)\n",
    "        loss = F.softmax_cross_entropy(pred, t)\n",
    "        acc = F.accuracy(pred, t)\n",
    "        \n",
    "        model.zerograd()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "    train_loss_saver.append((epoch, sum_loss / len(train_set)))\n",
    "    train_acc_saver.append((epoch, sum_acc / len(train_set)))\n",
    "    print(f\"[{epoch+1}/{max_epoch}] train_loss: {sum_loss / len(train_set)}, \"\n",
    "          f\"accuracy: {sum_acc / len(train_set)}\")\n",
    "    \n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with mytorch.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            pred = model(x)\n",
    "            loss = mytorch.functions.softmax_cross_entropy(pred, t)\n",
    "            acc = mytorch.functions.accuracy(pred, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "        test_loss_saver.append((epoch, sum_loss / len(test_set)))\n",
    "        test_acc_saver.append((epoch, sum_acc / len(test_set)))\n",
    "        print(f\"test_loss: {sum_loss / len(test_set)}, \"\n",
    "              f\"accuracy: {sum_acc / len(test_set)}\")\n",
    "    print(f\"elapse times : {time.time() - start}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
